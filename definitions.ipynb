{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalars, vectors, and matrices\n",
    "\n",
    "The bellow definitions are often seen in ML practise: \n",
    "\n",
    "## Scalars\n",
    "\n",
    "**Scalar** - a single number. They are denoted with lowercase letters. For example, \n",
    "\n",
    "a = 1\n",
    "\n",
    "b = 67.5\n",
    "\n",
    "c = -100\n",
    "\n",
    "$\\lambda = 0.01$\n",
    "\n",
    "## Vectors\n",
    "\n",
    "**Vector** - a collection of ordered numbers, denoted with brackets from each side. They are denoted with bolded lowercase letters. For example,\n",
    "\n",
    "$$\\textbf{v} = [v_{0}, v_{1}, ..., v_{n}]$$\n",
    "\n",
    "Usually, the vectors are defined as having one column and $n$ number of rows: \n",
    "\n",
    "$$\\textbf{v} = \\begin{bmatrix} v_{0} \\\\ v_{1} \\\\ ... \\\\ v_{n} \\end{bmatrix}_{N \\times 1}$$\n",
    "\n",
    "## Matrices \n",
    "\n",
    "**Matrix** - a rectangle table of numbers, denoted with brackets from each side. Each matrix element can be accessed by its row and column index. For example,\n",
    "\n",
    "$$\\mathbb{X} = \\begin{bmatrix} x_{11} & x_{12} & ... & x_{1p} \\\\ x_{21} & x_{22} & ... & x_{2p} \\\\ ... \\\\ x_{n1} & x_{n2} & ... & x_{np} \\end{bmatrix}_{n \\times p}$$\n",
    "\n",
    "## Tensor \n",
    "\n",
    "**Tensor** - An $n^{th}$-rank tensor in m-dimensional space is a mathematical object that has n indices and $m^{n}$ components and obeys certain transformation rules. It is a generalization of a matrix and a vector. For example, a 3d tensor has three coordinates that identify it's object in a 3d space. \n",
    "\n",
    "![](docs/tensor.png)\n",
    "\n",
    "The above tensor is a 3rd tensor where each scalar is defined with $x y z$ coordinates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions \n",
    "\n",
    "## Function definition\n",
    "\n",
    "A function in math is a rule that maps items from one set to another. \n",
    "\n",
    "The mathematical notation for a function is: \n",
    "\n",
    "$$f: \\mathbb{X} \\rightarrow \\mathbb{Y}$$\n",
    "\n",
    "The above notation maps any item from the set $\\mathbb{X}$ to the set $\\mathbb{Y}$.\n",
    "\n",
    "The sets $\\mathbb{X}$ and $\\mathbb{Y}$ are called the `domain` and `range` of the function, respectively.\n",
    "\n",
    "The $\\mathbb{X}$ set can hold any type of data: scalars, vectors, matrices, tensors, etc.  \n",
    "\n",
    "The $\\mathbb{Y}$ set in ML usually holds a scalar or vector.\n",
    "\n",
    "### Example\n",
    "\n",
    "A simple function is a function that maps one scalar to another scalar. For example, \n",
    "\n",
    "Lets us say that the $$\\mathbb{X} = \\mathbb{R}^{2}$$ (Each observation in the dataset is a 2d vector) and the $$\\mathbb{Y} = \\mathbb{R}^{1}$$ (The output is a scalar).\n",
    "\n",
    "$$ f(\\mathbb{X}) = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning (**ML**) - the process of an electronical system, given a metric of `goodness`, increase the metric value given more `data`. The key points here are `metric` and `data` - without them we cannot define any machine learning model.  \n",
    "\n",
    "![](docs/ml-high-level.png)\n",
    "\n",
    "The machine can now create rules based on the data and the evidence we give it. A programmer does not need to spend countless hours creating rules based on the data he sees - the computer can do that much more efficiently. \n",
    "\n",
    "Often, the definitions machine learning and machine learning model are used interchangebly. \n",
    "\n",
    "A machine learning model is a function that maps data to a scalar or vector: \n",
    "\n",
    "$$ f : \\mathbb{X}^{n} \\rightarrow \\mathbb{Y}$$\n",
    "\n",
    "Thus machine learning model $\\approx$ function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function \n",
    "\n",
    "At the heart of every machine learning algorithm is a `loss function` ussualy denoted as an uppercase $L$. The loss function measure how **good** are the predictions of the machine learning algorithm. For example, in a typical linear regression model, the loss function is the **mean squared error**: \n",
    "\n",
    "$$MSE = \\dfrac{1}{n} \\Sigma_{i=1}^{N} \\left(y_{i} - \\widehat{y_{i}} \\right)^{2}$$\n",
    "\n",
    "Here \n",
    "\n",
    "$i$ - the index of the observation \n",
    "\n",
    "$y_{i}$ - true observation i \n",
    "\n",
    "$\\widehat{y_{i}}$ - predicted observation i \n",
    "\n",
    "For binary classification problems, a popular loss function is the **binary cross entropy**:\n",
    "\n",
    "$$CE = - \\dfrac{1}{N} \\Sigma_{i}^{n}\\left( y_{i} log(p_{i}) + (1 + y_{i}) log(1 - p_{i}) \\right)$$\n",
    "\n",
    "Here \n",
    "\n",
    "$p_{i} = P(y_{i} = 1 | x_{i}) \\in (0, 1)$ \n",
    "\n",
    "In both the regression and the classification problems, the **smaller the loss, the better the model**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization \n",
    "\n",
    "The task of optimization in math is to find the arguments that either **minimize** or **maximize** a given function. In most ML algorithms, the general rule is to **minimize** the loss function. \n",
    "\n",
    "The notation for finding the arguments that minimize the loss function is:\n",
    "\n",
    "$$\\underset{x}{\\mathrm{argmin}} f(x)$$\n",
    "\n",
    "In machine learning, the data (meaning $\\mathbb{X}$ and $\\mathbb{Y}$) is fixed and the only thing that a model can change are the coefficients (sometimes called weights). Thus, the goal of the optimization is to find the coefficients that minimize the loss function. Additionaly, all the loss functions take coefficients as arguments. \n",
    "\n",
    "## Test case - predicting weight based on height for the NBA players\n",
    "\n",
    "The data is taken from https://www.kaggle.com/datasets/justinas/nba-players-data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data reading package \n",
    "import pandas as pd \n",
    "\n",
    "# Ploting \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data \n",
    "d = pd.read_csv('data/nba-data.csv')\n",
    "print(f\"Number of observations: {d.shape[0]}\")\n",
    "print(f\"{d.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting the relationship between player_height and player_weight\n",
    "d.plot(x='player_height', y='player_weight', kind='scatter', figsize=(10,10))\n",
    "plt.title('Height vs Weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a clear linear relationship here: the bigger the height, the higher the weight. We will try to fit a linear model to the data: \n",
    "\n",
    "$$ w = \\beta_{0} + \\beta_{1}h $$ \n",
    "\n",
    "Here \n",
    "\n",
    "$w$ - the weight\n",
    "\n",
    "$h$ - the height\n",
    "\n",
    "$\\beta_{0}$ - the intercept (average weight)\n",
    "\n",
    "$\\beta_{1}$ - the slope (height to weight)\n",
    "\n",
    "## Constructing the machine learning problem \n",
    "\n",
    "In order to solve the problem, we need to define the data, the model, the loss function and an optimization algorithm for the loss function. \n",
    "\n",
    "The data is the height and the weight of the players in the NBA: \n",
    "\n",
    "$$ \\mathbb{D} := \\{h_{i}, w_{i}\\} $$ \n",
    "\n",
    "$$ i \\in \\{1, 2, ..., 11700\\} $$\n",
    "\n",
    "The model: \n",
    "\n",
    "$$ w_{i} = \\beta_{0} + \\beta_{1}h_{i} $$ \n",
    "\n",
    "The loss function is the **mean squared error**:\n",
    "\n",
    "$$L(\\beta_{0}, \\beta_{1}) = \\dfrac{1}{n} \\Sigma_{i=1}^{N} \\left(w_{i} - (\\beta_{0} + \\beta_{1} h_{i}) \\right)^{2}$$ \n",
    "\n",
    "The optimization algorithm is the **gradient descent**. \n",
    "\n",
    "### Gradient descent \n",
    "\n",
    "Gradient descent's main idea is that any differentiable function decreases with a given input $x$ if the derivative of the function with the value $x$ is negative. \n",
    "\n",
    "The the algorithm is an iterative algorithm where at each step, the coefficients $\\beta$ get updated based on the derivative of the loss function with the current coefficients: \n",
    "\n",
    "We set a number of iterations $M$ and the learning rate $\\alpha > 0$. \n",
    "\n",
    "Then, for m = 0 to M, we update the coefficients $\\beta$ as follows:\n",
    "\n",
    "$$ \\beta_{0}^{m + 1} \\leftarrow \\beta_{0}^{m} - \\alpha \\dfrac{\\partial L}{\\partial \\beta_{0}}$$\n",
    "\n",
    "$$ \\beta_{1}^{m + 1} \\leftarrow \\beta_{1}^{m} - \\alpha \\dfrac{\\partial L}{\\partial \\beta_{1}}$$\n",
    "\n",
    "Here \n",
    "\n",
    "$m$ - iteration number \n",
    "\n",
    "In order to implement the algorithm, we need to define the partial derivatives. \n",
    "\n",
    "$$\\dfrac{\\partial L}{\\partial \\beta_{0}} = \\dfrac{1}{N} \\Sigma_{i=1}^{N} -2 (w_{i} - (\\beta_{0} + \\beta_{1} h_{i}))$$\n",
    "\n",
    "$$\\dfrac{\\partial L}{\\partial \\beta_{1}} = \\dfrac{1}{N} \\Sigma_{i=1}^{N} -2 h_{i} (w_{i} - (\\beta_{0} + \\beta_{1} h_{i}))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(x, y, beta_0, beta_1):\n",
    "    \"\"\"\n",
    "    Mean squared error implementation\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    mse = 0\n",
    "    for i in range(n):\n",
    "        mse += (y[i] - (beta_0 + beta_1 * x[i])) ** 2\n",
    "    return mse / n\n",
    "\n",
    "def gradient_update(x, y, beta_0, beta_1, alpha): \n",
    "    \"\"\"\n",
    "    Weight update based on gradient implementation for 1 epoch\n",
    "    \"\"\"\n",
    "    dL_d0 = 0\n",
    "    dL_d1 = 0\n",
    "\n",
    "    # Saving the number of obs \n",
    "    n = len(x)\n",
    "\n",
    "    # Getting the sums \n",
    "    for i in range(n):\n",
    "        dL_d0 += -2 * (y[i] - (beta_0 + beta_1 * x[i]))\n",
    "        dL_d1 += -2 * x[i] * (y[i] - (beta_0 + beta_1 * x[i]))\n",
    "\n",
    "    # Getting the means \n",
    "    dL_d0 /= n\n",
    "    dL_d1 /= n\n",
    "\n",
    "    # Updating the betas\n",
    "    beta_0 = beta_0 - alpha * dL_d0\n",
    "    beta_1 = beta_1 - alpha * dL_d1\n",
    "\n",
    "    # Returning the updated weights \n",
    "    return beta_0, beta_1\n",
    "\n",
    "def gradient_descent(x, y, beta_0, beta_1, alpha, epochs):\n",
    "    \"\"\"\n",
    "    Gradient descent implementation for multiple epochs\n",
    "    \"\"\"\n",
    "    for i in range(epochs):\n",
    "        beta_0, beta_1 = gradient_update(x, y, beta_0, beta_1, alpha)\n",
    "\n",
    "        # Logging the loss after each 100 epochs \n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch {i}: {MSE(x, y, beta_0, beta_1)}\")\n",
    "    return beta_0, beta_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data standartization \n",
    "\n",
    "Gradient descent works best when the data is standardized. To ensure that the gradient descent moves smoothly towards the minima and that the steps for gradient descent are updated at the same rate for all the features, we scale the data before feeding it to the model. A popular standartization technique is the **mean-variance**:\n",
    "\n",
    "We transform every entry in $x$ by subtracting the mean and dividing by its standard deviation. \n",
    "\n",
    "$$ z_{i} = \\dfrac{x_{i} - \\overline{x}}{\\sigma(x)}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the x variable\n",
    "d = d.sort_values(by='player_height')\n",
    "\n",
    "# Standatizing the data \n",
    "x = d['player_height']\n",
    "y = d['player_weight']\n",
    "\n",
    "# Importing the scalers \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "_x_scaler = StandardScaler()\n",
    "_y_scaler = StandardScaler()\n",
    "\n",
    "# Applying \n",
    "z_x = _x_scaler.fit_transform(x.values.reshape(-1, 1))\n",
    "z_y = _y_scaler.fit_transform(y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for the best parameters \n",
    "alpha = 0.1\n",
    "epochs = 100\n",
    "beta_0 = 0\n",
    "beta_1 = 0\n",
    "\n",
    "_beta_0, _beta_1 = gradient_descent(x=z_x, y=z_y, beta_0=beta_0, beta_1=beta_1, alpha=alpha, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Gradient descent results: {_beta_0}, {_beta_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predictions \n",
    "_predictions_gd = _beta_0 + _beta_1 * z_x\n",
    "\n",
    "# Scaling back \n",
    "_predictions_gd = _y_scaler.inverse_transform(_predictions_gd.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression in scikit learn \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(z_x, z_y)\n",
    "\n",
    "# Getting the coefficients\n",
    "print(f\"Scikit learn results: {lr.intercept_}, {lr.coef_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predictions \n",
    "predictions = lr.predict(z_x)\n",
    "\n",
    "# Inverse transforming \n",
    "predictions = _y_scaler.inverse_transform(predictions.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting the true and predicted values\n",
    "d.plot(x='player_height', y='player_weight', kind='scatter', figsize=(10,10))\n",
    "plt.title('Height vs Weight')\n",
    "plt.plot(x, predictions, 'r', label='Predicted - Linear Regression')\n",
    "plt.plot(x, _predictions_gd, 'g', label='Predicted - Gradient Descent')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('ml_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "842abcfcfa397b81a8d168d89db07cbc6234dad32ee17856ea8adce73f4f135e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
