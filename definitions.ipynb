{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Array math\n",
    "import numpy \n",
    "\n",
    "# Data reading package \n",
    "import pandas as pd \n",
    "\n",
    "# Interactions \n",
    "from ipywidgets import interact, widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalars, vectors, and matrices\n",
    "\n",
    "The bellow definitions are often seen in ML practise: \n",
    "\n",
    "## Scalars\n",
    "\n",
    "**Scalar** - a single number. They are denoted with lowercase letters. For example, \n",
    "\n",
    "a = 1\n",
    "\n",
    "b = 67.5\n",
    "\n",
    "c = -100\n",
    "\n",
    "$\\lambda = 0.01$\n",
    "\n",
    "## Vectors\n",
    "\n",
    "**Vector** - a collection of ordered numbers, denoted with brackets from each side. They are denoted with bolded lowercase letters. For example,\n",
    "\n",
    "$$\\textbf{v} = [v_{0}, v_{1}, ..., v_{n}]$$\n",
    "\n",
    "Usually, the vectors are defined as having one column and $n$ number of rows: \n",
    "\n",
    "$$\\textbf{v} = \\begin{bmatrix} v_{0} \\\\ v_{1} \\\\ ... \\\\ v_{n} \\end{bmatrix}_{N \\times 1}$$\n",
    "\n",
    "## Matrices \n",
    "\n",
    "**Matrix** - a rectangle table of numbers, denoted with brackets from each side. Each matrix element can be accessed by its row and column index. For example,\n",
    "\n",
    "$$\\mathbb{X} = \\begin{bmatrix} x_{11} & x_{12} & ... & x_{1p} \\\\ x_{21} & x_{22} & ... & x_{2p} \\\\ ... \\\\ x_{n1} & x_{n2} & ... & x_{np} \\end{bmatrix}_{n \\times p}$$\n",
    "\n",
    "## Tensor \n",
    "\n",
    "**Tensor** - An $n^{th}$-rank tensor in m-dimensional space is a mathematical object that has n indices and $m^{n}$ components and obeys certain transformation rules. It is a generalization of a matrix and a vector. For example, a 3d tensor has three coordinates that identify it's object in a 3d space. \n",
    "\n",
    "![](docs/tensor.png)\n",
    "\n",
    "The above tensor is a 3rd tensor where each scalar is defined with $x y z$ coordinates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions \n",
    "\n",
    "## Function definition\n",
    "\n",
    "A function in math is a rule that maps items from one set to another. \n",
    "\n",
    "The mathematical notation for a function is: \n",
    "\n",
    "$$f: \\mathbb{X} \\rightarrow \\mathbb{Y}$$\n",
    "\n",
    "The above notation maps any item from the set $\\mathbb{X}$ to the set $\\mathbb{Y}$.\n",
    "\n",
    "The sets $\\mathbb{X}$ and $\\mathbb{Y}$ are called the `domain` and `range` of the function, respectively.\n",
    "\n",
    "The $\\mathbb{X}$ set can hold any type of data: scalars, vectors, matrices, tensors, etc.  \n",
    "\n",
    "The $\\mathbb{Y}$ set in ML usually holds a scalar or vector.\n",
    "\n",
    "### Example\n",
    "\n",
    "A simple function is a function that maps one scalar to another scalar. For example, \n",
    "\n",
    "Lets us say that the $$\\mathbb{X} = \\mathbb{R}^{2}$$ (Each observation in the dataset is a 2d vector) and the $$\\mathbb{Y} = \\mathbb{R}^{1}$$ (The output is a scalar).\n",
    "\n",
    "A linear with two arguments:\n",
    "\n",
    "$$ f(\\mathbb{X}) = f(x_{1}, x_{2}) = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning (**ML**) - the process of increasing a metric of `goodness` by having more `data` done by an electronical system. The key points here are `metric` and `data` - without them we cannot define any machine learning model.  \n",
    "\n",
    "![](docs/ml-high-level.png)\n",
    "\n",
    "The machine can now create rules based on the data and the evidence we give it. A programmer does not need to spend countless hours creating rules based on the data he sees - the computer can do that much more efficiently. \n",
    "\n",
    "Often, the definitions machine learning and machine learning model are used interchangebly. \n",
    "\n",
    "A machine learning model is a function that maps data to a scalar or vector: \n",
    "\n",
    "$$ f : \\mathbb{X}^{n} \\rightarrow \\mathbb{Y}$$\n",
    "\n",
    "Thus machine learning model $\\approx$ function. \n",
    "\n",
    "## Gradient \n",
    "\n",
    "The gradient is the vector of all the partial derivatives of the function with respect to each of the parameters. It is commonly denoted as $\\nabla f$. If \n",
    "\n",
    "$$f: X^{p} \\rightarrow Y $$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\\nabla f = \\left(\\dfrac{\\partial f}{\\partial x_{1}}, \\dfrac{\\partial f}{\\partial x_{2}}, ..., \\dfrac{\\partial f }{\\partial x_{p}} \\right) $$\n",
    "\n",
    "The most important feature of a gradient is that, losely speaking, when it is negative, the original function is decreasing. This attribute helps tremendously in the machine learning process.\n",
    "\n",
    "## Optimization \n",
    "\n",
    "The task of optimization in math is to find the arguments that either **minimize** or **maximize** a given function. In most ML algorithms, the general rule is to **minimize** the loss function. \n",
    "\n",
    "The notation for finding the arguments that minimize the loss function is:\n",
    "\n",
    "$$\\underset{x}{\\mathrm{argmin}} f(x)$$\n",
    "\n",
    "When optimizing a function, the gradient helps to find the optimal arguments. \n",
    "\n",
    "## Optimization example - gradient descent\n",
    "\n",
    "Let us say we have a function: \n",
    "\n",
    "$$ f(x) = x^{2}$$\n",
    "\n",
    "The gradient is: \n",
    "\n",
    "$$ \\nabla f = \\dfrac{\\partial f}{\\partial x} = 2x $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function \n",
    "def f(x: float) -> float: \n",
    "    \"\"\"\n",
    "    Original function\n",
    "    \"\"\"\n",
    "    return x**2\n",
    "\n",
    "def d_f(x: float) -> float: \n",
    "    \"\"\"\n",
    "    Derivative\n",
    "    \"\"\"\n",
    "    return 2*x\n",
    "\n",
    "# Creating the x range \n",
    "x_range = numpy.linspace(-10, 10, 100)\n",
    "\n",
    "# Getting the y values \n",
    "y_values = f(x_range)\n",
    "\n",
    "# Ploting the graph\n",
    "plt.plot(x_range, y_values)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Simple parabola')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the derivative values \n",
    "d_y_values = d_f(x_range)\n",
    "\n",
    "# Ploting \n",
    "plt.plot(x_range, d_y_values)\n",
    "# Adding a horizontal line at 0\n",
    "plt.axhline(y=0, color='black')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Derivative of the parabola')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that whenever the gradient is negative, the function is decreasing.\n",
    "\n",
    "`Gradient descent` is an algorithm where the main idea is that we can move to the direction of the negative gradient when finding the $x^{*}$ that optimizes the original function. \n",
    "\n",
    "In our parabola case, the algorithm is as follows: \n",
    "\n",
    "1) Define the number of iterations $M$. \n",
    "\n",
    "2) Define the learning rate $\\alpha \\in (0, \\infty)$.\n",
    "\n",
    "3) Initialize the parameter $x^{0}$ to some random value.\n",
    "\n",
    "4) For $i = 0, \\dots, M - 1$:\n",
    "    \n",
    "    $x^{i + 1} = x^{i} - \\alpha \\nabla f(x^{i}) =  x^{i} - \\alpha 2 x^{i}$\n",
    "\n",
    "Lets see the optimization in action:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the min and max learning rate \n",
    "min_alpha = 0.01\n",
    "max_alpha = 1\n",
    "\n",
    "def gradient_descent_parabola(m: int, alpha: float, x: float) -> list: \n",
    "    # Initializing the list to store the x values\n",
    "    x_values = []\n",
    "\n",
    "    # Starting the loop\n",
    "    for _ in range(m):\n",
    "        # Appending the x value to the list\n",
    "        x_values.append(x)\n",
    "\n",
    "        # Updating the x value\n",
    "        x = x - alpha*d_f(x)\n",
    "\n",
    "    # Ploting the original parabola \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(x_range, f(x_range))\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Gradient descent')\n",
    "\n",
    "    #Adding the gradient descent points \n",
    "    plt.plot(x_values, [x ** 2 for x in x_values], 'ro')\n",
    "\n",
    "    # Adding the labels to the points indicating the iteration it was calculated at\n",
    "    for i, x in enumerate(x_values):\n",
    "        plt.annotate(i, (x, x ** 2 + 1))\n",
    "\n",
    "    # Showing the plot \n",
    "    plt.show()\n",
    "\n",
    "interact(\n",
    "    gradient_descent_parabola, \n",
    "    m=widgets.IntSlider(min=1, max=100, step=1, value=10), \n",
    "    alpha=widgets.FloatSlider(min=min_alpha, max=max_alpha, step=0.01, value=0.1),\n",
    "    x=widgets.FloatSlider(min=-10, max=10, step=0.01, value=8)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, after each iteration, x keeps on getting closer to the minimum of the function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function \n",
    "\n",
    "At the heart of every machine learning algorithm is a `loss function` ussualy denoted as an uppercase $L$. The loss function measure how **good** are the predictions of the machine learning algorithm. For example, in a typical linear regression model, the loss function is the **mean squared error**: \n",
    "\n",
    "$$MSE = \\dfrac{1}{n} \\Sigma_{i=1}^{N} \\left(y_{i} - \\widehat{y_{i}} \\right)^{2}$$\n",
    "\n",
    "Here \n",
    "\n",
    "$i$ - the index of the observation \n",
    "\n",
    "$y_{i}$ - true observation i \n",
    "\n",
    "$\\widehat{y_{i}}$ - predicted observation i \n",
    "\n",
    "For binary classification problems, a popular loss function is the **binary cross entropy**:\n",
    "\n",
    "$$CE = - \\dfrac{1}{N} \\Sigma_{i}^{n}\\left( y_{i} log(p_{i}) + (1 + y_{i}) log(1 - p_{i}) \\right)$$\n",
    "\n",
    "Here \n",
    "\n",
    "$p_{i} = P(y_{i} = 1 | x_{i}) \\in (0, 1)$ \n",
    "\n",
    "In both the regression and the classification problems, the **smaller the loss, the better the model**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test case - predicting weight based on height for the NBA players\n",
    "\n",
    "The data is taken from https://www.kaggle.com/datasets/justinas/nba-players-data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data \n",
    "d = pd.read_csv('data/nba-data.csv')\n",
    "print(f\"Number of observations: {d.shape[0]}\")\n",
    "print(f\"{d.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting the relationship between player_height and player_weight\n",
    "d.plot(x='player_height', y='player_weight', kind='scatter', figsize=(10,10))\n",
    "plt.title('Height vs Weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a clear linear relationship here: the bigger the height, the higher the weight. We will try to fit a linear model to the data: \n",
    "\n",
    "$$ w = \\beta_{0} + \\beta_{1}h $$ \n",
    "\n",
    "Here \n",
    "\n",
    "$w$ - the weight\n",
    "\n",
    "$h$ - the height\n",
    "\n",
    "$\\beta_{0}$ - the intercept (average weight)\n",
    "\n",
    "$\\beta_{1}$ - the slope (height to weight)\n",
    "\n",
    "## Constructing the machine learning problem \n",
    "\n",
    "In order to solve the problem, we need to define the data, the model, the loss function and an optimization algorithm for the loss function. \n",
    "\n",
    "The data is the height and the weight of the players in the NBA: \n",
    "\n",
    "$$ \\mathbb{D} := \\{h_{i}, w_{i}\\} $$ \n",
    "\n",
    "$$ i \\in \\{1, 2, ..., 11700\\} $$\n",
    "\n",
    "The model: \n",
    "\n",
    "$$ w_{i} = \\beta_{0} + \\beta_{1}h_{i} $$ \n",
    "\n",
    "The loss function is the **mean squared error**:\n",
    "\n",
    "$$L(\\beta_{0}, \\beta_{1}) = \\dfrac{1}{n} \\Sigma_{i=1}^{N} \\left(w_{i} - (\\beta_{0} + \\beta_{1} h_{i}) \\right)^{2}$$ \n",
    "\n",
    "The optimization algorithm is the **gradient descent**. \n",
    "\n",
    "The the algorithm is an iterative algorithm where at each step, the coefficients $\\beta$ get updated based on the derivative of the loss function with the current coefficients: \n",
    "\n",
    "We set a number of iterations $M$ and the learning rate $\\alpha > 0$. \n",
    "\n",
    "Then, for m = 0 to M, we update the coefficients $\\beta$ as follows:\n",
    "\n",
    "$$ \\beta_{0}^{m + 1} \\leftarrow \\beta_{0}^{m} - \\alpha \\dfrac{\\partial L}{\\partial \\beta_{0}}$$\n",
    "\n",
    "$$ \\beta_{1}^{m + 1} \\leftarrow \\beta_{1}^{m} - \\alpha \\dfrac{\\partial L}{\\partial \\beta_{1}}$$\n",
    "\n",
    "Here \n",
    "\n",
    "$m$ - iteration number \n",
    "\n",
    "In order to implement the algorithm, we need to define the partial derivatives. \n",
    "\n",
    "$$\\dfrac{\\partial L}{\\partial \\beta_{0}} = \\dfrac{1}{N} \\Sigma_{i=1}^{N} -2 (w_{i} - (\\beta_{0} + \\beta_{1} h_{i}))$$\n",
    "\n",
    "$$\\dfrac{\\partial L}{\\partial \\beta_{1}} = \\dfrac{1}{N} \\Sigma_{i=1}^{N} -2 h_{i} (w_{i} - (\\beta_{0} + \\beta_{1} h_{i}))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(x, y, beta_0, beta_1):\n",
    "    \"\"\"\n",
    "    Mean squared error implementation\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    mse = 0\n",
    "    for i in range(n):\n",
    "        mse += (y[i] - (beta_0 + beta_1 * x[i])) ** 2\n",
    "    return mse / n\n",
    "\n",
    "def gradient_update(x, y, beta_0, beta_1, alpha): \n",
    "    \"\"\"\n",
    "    Weight update based on gradient implementation for 1 epoch\n",
    "    \"\"\"\n",
    "    dL_d0 = 0\n",
    "    dL_d1 = 0\n",
    "\n",
    "    # Saving the number of obs \n",
    "    n = len(x)\n",
    "\n",
    "    # Getting the sums \n",
    "    for i in range(n):\n",
    "        dL_d0 += -2 * (y[i] - (beta_0 + beta_1 * x[i]))\n",
    "        dL_d1 += -2 * x[i] * (y[i] - (beta_0 + beta_1 * x[i]))\n",
    "\n",
    "    # Getting the means \n",
    "    dL_d0 /= n\n",
    "    dL_d1 /= n\n",
    "\n",
    "    # Updating the betas\n",
    "    beta_0 = beta_0 - alpha * dL_d0\n",
    "    beta_1 = beta_1 - alpha * dL_d1\n",
    "\n",
    "    # Returning the updated weights \n",
    "    return beta_0, beta_1\n",
    "\n",
    "def gradient_descent(x, y, beta_0, beta_1, alpha, epochs):\n",
    "    \"\"\"\n",
    "    Gradient descent implementation for multiple epochs\n",
    "    \"\"\"\n",
    "    for i in range(epochs):\n",
    "        beta_0, beta_1 = gradient_update(x, y, beta_0, beta_1, alpha)\n",
    "\n",
    "        # Logging the loss after each 100 epochs \n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch {i}: {MSE(x, y, beta_0, beta_1)}\")\n",
    "    return beta_0, beta_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data standartization \n",
    "\n",
    "Gradient descent works best when the data is standardized. To ensure that the gradient descent moves smoothly towards the minima and that the steps for gradient descent are updated at the same rate for all the features, we scale the data before feeding it to the model. A popular standartization technique is the **mean-variance**:\n",
    "\n",
    "We transform every entry in $x$ by subtracting the mean and dividing by its standard deviation. \n",
    "\n",
    "$$ z_{i} = \\dfrac{x_{i} - \\overline{x}}{\\sigma(x)}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the x variable\n",
    "d = d.sort_values(by='player_height')\n",
    "\n",
    "# Standatizing the data \n",
    "x = d['player_height']\n",
    "y = d['player_weight']\n",
    "\n",
    "# Importing the scalers \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "_x_scaler = StandardScaler()\n",
    "_y_scaler = StandardScaler()\n",
    "\n",
    "# Applying \n",
    "z_x = _x_scaler.fit_transform(x.values.reshape(-1, 1))\n",
    "z_y = _y_scaler.fit_transform(y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for the best parameters \n",
    "alpha = 0.1\n",
    "epochs = 100\n",
    "beta_0 = 0\n",
    "beta_1 = 0\n",
    "\n",
    "_beta_0, _beta_1 = gradient_descent(x=z_x, y=z_y, beta_0=beta_0, beta_1=beta_1, alpha=alpha, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Gradient descent results: {_beta_0}, {_beta_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predictions \n",
    "_predictions_gd = _beta_0 + _beta_1 * z_x\n",
    "\n",
    "# Scaling back \n",
    "_predictions_gd = _y_scaler.inverse_transform(_predictions_gd.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression in scikit learn \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(z_x, z_y)\n",
    "\n",
    "# Getting the coefficients\n",
    "print(f\"Scikit learn results: {lr.intercept_}, {lr.coef_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predictions \n",
    "predictions = lr.predict(z_x)\n",
    "\n",
    "# Inverse transforming \n",
    "predictions = _y_scaler.inverse_transform(predictions.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting the true and predicted values\n",
    "d.plot(x='player_height', y='player_weight', kind='scatter', figsize=(10,10))\n",
    "plt.title('Height vs Weight')\n",
    "plt.plot(x, predictions, 'r', label='Predicted - Linear Regression')\n",
    "plt.plot(x, _predictions_gd, 'g', label='Predicted - Gradient Descent')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('ml_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "842abcfcfa397b81a8d168d89db07cbc6234dad32ee17856ea8adce73f4f135e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
